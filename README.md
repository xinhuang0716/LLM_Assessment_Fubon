## Task

1. 設計評測指標：安全性、可靠性（穩定性）、可解釋性、透明性
2. 建立測試資料集與場景模擬
3. 開發自動化測試工具（可用 Python + LangChain + pytest 等）
4. 建立報告產出機制（每次測試自動生成報告）

## Milestone
1. 完成評測框架設計
2. 建立自動化測試原型並驗證可行性

  [註] 範疇限定在LLM模型本身，不擴大至其他LLM服務或Application的評測

## Schedule

#### # 專案啟動9/16
 
1. 【LLM評測指標】調研：LLM模型評測維度(安全性、可靠性、可解釋性、透明性)下的衡量指標 --黃昕/宥任/御唐
    - LLM模型評測維度(安全性、可靠性、可解釋性、透明性)下的衡量指標有哪些?
    - 各項衡量指標的計算方式?
    - 各項衡量指標若要開發成測試工具，是否有現成的OPEN SOURCE套件或自主開發? 在技術文件中，input資料等級所需的計算時間?...等
 
2. 【模型盤點】盤點現有四大BU及證券場景下，所使用到的LLM模型有哪些? --君綺 TBD 跟偉瀚Align一下
 
#### # 第一次報告9/30，於9/26先組內彙整
 
3. 決議所適用的【LLM評測指標】，包括: 入選指標、計算方式、套件.. -> 鼎文 家斌 君綺 黃昕/宥任/御唐 + Milestone 與其他兩組Align
4. 各自依據所負責的評測維度及評測指標，設計及撰寫【評測框架】-> 黃昕/宥任/御唐
 
#### # 第二次報告10/30
 
5. 彙整及定版【評測框架】，產出最終完整版的評測報告 -> 鼎文 家斌 君綺 黃昕/宥任
6. 開發評測python程式 -> 黃昕/宥任/御唐
 
#### # 第三次報告11/30
 
7. 【開發自動化工具】打包及建立自動化測試工具，若自動化上有碰到窒礙難行的地方再共同討論 -> TBD
8. 【實測自動化工具】選用新模型(未在工項2中的模型)進行評測 -> TBD
9. 【開發自動化工具】、【實測自動化工具】接續工項8、工項9，蒐集及整理自動化測試工具的評測結果及優化紀錄 --黃昕/宥任/御唐

